{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df72150",
   "metadata": {},
   "source": [
    "# Exercise 2: Train, dev(validation), Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6c1c6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt','r') as r:\n",
    "    words = r.read()\n",
    "    words = words.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "19d4c045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a04ed1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = words[:int(len(words)* 0.8)]\n",
    "test = words[int(len(words)* 0.8):int(len(words)* 0.9)]\n",
    "val = words[int(len(words)* 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c260d53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80% training set\n",
      "20% validation set\n",
      "10% test set\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(train)/len(words)*100:.0f}% training set')\n",
    "print(f'{len(val)/len(words)*100:.0f}% validation set')\n",
    "print(f'{len(test)/len(words)*100:.0f}% test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d8a36e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars  = sorted(set(''.join(words)))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "0067bded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos = {i:s for s,i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "9871474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for w in words:\n",
    "    chars = ['.'] +list(w)+ ['.']\n",
    "    for char1,char2,char3 in zip(chars[:],chars[1:],chars[2:]):\n",
    "        ix1 = stoi[char1]\n",
    "        ix2 = stoi[char2]\n",
    "        ix3 = stoi[char3]\n",
    "        x.append([ix1,ix2])\n",
    "        y.append(ix3)\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0004255c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196113, 2])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f7f25c",
   "metadata": {},
   "source": [
    "# Lets split our data into train,test and val. We use stratify = True to preserve the same class distribution as the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9e882683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_temp,y_train, y_temp = train_test_split(x,y,test_size=.2,random_state=40,shuffle=True,stratify=y)\n",
    "x_test, x_val,y_test,y_val = train_test_split(x_temp,y_temp,test_size=.5,random_state=40,shuffle=True,stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "364c50f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([156890, 2]) torch.Size([19612, 2]) torch.Size([19611, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,x_val.shape,x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b001126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(5)\n",
    "import torch.nn.functional as F\n",
    "def data_prep(x:torch.tensor,y:torch.tensor):\n",
    "    xenc = F.one_hot(x,num_classes=len(stoi)).float()\n",
    "    yenc = F.one_hot(y,num_classes=len(stoi)).float()\n",
    "\n",
    "    xflat = xenc.view(-1,27*2)\n",
    "\n",
    "    return xflat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "94a92827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prep(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e0fd51",
   "metadata": {},
   "source": [
    "# Lets now run a few training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "06785dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gradient descent with 250 steps: 100%|\u001b[32m██████████\u001b[0m| 250/250 [00:04<00:00, 56.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative_Log_Likelihood_loss after 250 runs is: 2.2428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "epochs = 250\n",
    "xenc = data_prep(x_train,y_train)\n",
    "\n",
    "w = torch.rand((27*2,27),generator=g,requires_grad=True)\n",
    "\n",
    "for n in trange(epochs,desc=f'Running gradient descent with {epochs} steps',colour='green'):\n",
    "\n",
    "    logits = xenc @ w\n",
    "\n",
    "    counts = logits.exp()\n",
    "\n",
    "    probs = counts / counts.sum(1,keepdim=True)\n",
    "\n",
    "    loss = -probs[torch.arange(len(y_train)),y_train].log().mean()\n",
    "\n",
    "    w.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    w.data += -(50*w.grad)\n",
    "\n",
    "    n+=1\n",
    "\n",
    "print(f'Negative_Log_Likelihood_loss after {n} runs is: {loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "ae232d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on our validation and test set is 2.2510 and 2.2554\n"
     ]
    }
   ],
   "source": [
    "x_val_enc = data_prep(x_val,y_val)\n",
    "logits = x_val_enc @ w\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1,keepdim=True)\n",
    "val_loss = -probs[torch.arange(len(y_val)),y_val].log().mean()\n",
    "\n",
    "x_test_enc = data_prep(x_test,y_test)\n",
    "logits = x_test_enc @ w\n",
    "counts = logits.exp() \n",
    "probs = counts / counts.sum(1,keepdim=True)\n",
    "test_loss = -probs[torch.arange(len(y_test)),y_test].log().mean()\n",
    "\n",
    "\n",
    "print(f\"The loss on our validation and test set is {val_loss:.4f} and {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe7c9eb",
   "metadata": {},
   "source": [
    "# Exercise 3: Finding a good regularisation parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "5d7f3e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When x is 0.12, the test loss is 2.3792\n",
      "When x is 0.01, the test loss is 2.2678\n",
      "When x is 0.00, the test loss is 2.2567\n",
      "When x is 0.00, the test loss is 2.2556\n",
      "When x is 1.24, the test loss is 3.4928\n"
     ]
    }
   ],
   "source": [
    "x_test_enc = data_prep(x_test,y_test)\n",
    "logits = x_test_enc @ w\n",
    "counts = logits.exp() \n",
    "probs = counts / counts.sum(1,keepdim=True)\n",
    "def regularisation_term(w,x):\n",
    "    return x*(w**2).mean()\n",
    "\n",
    "for x in [0.1,0.01,0.001,0.0001,1]:\n",
    "    i = regularisation_term(w,x)\n",
    "    test_loss = -probs[torch.arange(len(y_test)),y_test].log().mean() + i\n",
    "    print(f\"When x is {i:.2f}, the test loss is {test_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "ad49c6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gradient descent with 250 steps: 100%|\u001b[32m██████████\u001b[0m| 250/250 [00:04<00:00, 57.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative_Log_Likelihood_loss after 250 runs is: 2.2428\n",
      "Negative_Log_Likelihood_loss on validattion set is: 2.2511 when lambda is 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gradient descent with 250 steps: 100%|\u001b[32m██████████\u001b[0m| 250/250 [00:04<00:00, 59.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative_Log_Likelihood_loss after 250 runs is: 2.5449\n",
      "Negative_Log_Likelihood_loss on validattion set is: 2.3951 when lambda is 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gradient descent with 250 steps: 100%|\u001b[32m██████████\u001b[0m| 250/250 [00:04<00:00, 58.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative_Log_Likelihood_loss after 250 runs is: 2.3185\n",
      "Negative_Log_Likelihood_loss on validattion set is: 2.2707 when lambda is 0.10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gradient descent with 250 steps: 100%|\u001b[32m██████████\u001b[0m| 250/250 [00:04<00:00, 60.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative_Log_Likelihood_loss after 250 runs is: 2.2554\n",
      "Negative_Log_Likelihood_loss on validattion set is: 2.2524 when lambda is 0.01000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gradient descent with 250 steps: 100%|\u001b[32m██████████\u001b[0m| 250/250 [00:04<00:00, 57.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative_Log_Likelihood_loss after 250 runs is: 2.2442\n",
      "Negative_Log_Likelihood_loss on validattion set is: 2.2511 when lambda is 0.00100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gradient descent with 250 steps: 100%|\u001b[32m██████████\u001b[0m| 250/250 [00:04<00:00, 60.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative_Log_Likelihood_loss after 250 runs is: 2.2428\n",
      "Negative_Log_Likelihood_loss on validattion set is: 2.2510 when lambda is 0.00010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gradient descent with 250 steps: 100%|\u001b[32m██████████\u001b[0m| 250/250 [00:04<00:00, 60.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative_Log_Likelihood_loss after 250 runs is: 2.2428\n",
      "Negative_Log_Likelihood_loss on validattion set is: 2.2511 when lambda is 0.00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gradient descent with 250 steps: 100%|\u001b[32m██████████\u001b[0m| 250/250 [00:04<00:00, 58.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative_Log_Likelihood_loss after 250 runs is: 2.2427\n",
      "Negative_Log_Likelihood_loss on validattion set is: 2.2510 when lambda is 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0] + [1/10**i for i in range(7)]\n",
    "g = torch.Generator().manual_seed(5)\n",
    "weights = {}\n",
    "\n",
    "for l in lambdas:\n",
    "\n",
    "    from tqdm import trange\n",
    "    epochs = 250\n",
    "    xenc = data_prep(x_train,y_train)\n",
    "\n",
    "    w = torch.rand((27*2,27),generator=g,requires_grad=True)\n",
    "\n",
    "    for n in trange(epochs,desc=f'Running gradient descent with {epochs} steps',colour='green'):\n",
    "\n",
    "        logits = xenc @ w\n",
    "\n",
    "        counts = logits.exp()\n",
    "\n",
    "        probs = counts / counts.sum(1,keepdim=True)\n",
    "\n",
    "        loss = -probs[torch.arange(len(y_train)),y_train].log().mean() + l*(w**2).mean()\n",
    "\n",
    "        w.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        w.data += -(50*w.grad)\n",
    "\n",
    "        n+=1\n",
    "\n",
    "    x_test_enc = data_prep(x_val,y_val)\n",
    "    logits = x_test_enc @ w\n",
    "    counts = logits.exp() \n",
    "    probs = counts / counts.sum(1,keepdim=True)\n",
    "    test_loss = -probs[torch.arange(len(y_val)),y_val].log().mean()\n",
    "\n",
    "    weights[l] = [w,test_loss]\n",
    "\n",
    "    print(f'Negative_Log_Likelihood_loss after {n} runs is: {loss:.4f}')\n",
    "    print(f'Negative_Log_Likelihood_loss on validattion set is: {test_loss :.4f} when lambda is {l:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "5767b103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best λ : 0.0001\n",
      "dev CE : 2.2509894371032715\n"
     ]
    }
   ],
   "source": [
    "best_lam, (best_w, best_loss_tensor) = min(\n",
    "    weights.items(),\n",
    "    key=lambda kv: kv[1][1].item()   # kv[1] == [w, loss]; kv[1][1] is the loss tensor\n",
    ")\n",
    "\n",
    "best_loss = best_loss_tensor.item()\n",
    "print(\"best λ :\", best_lam)\n",
    "print(\"dev CE :\", best_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "86fc6945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test loss where λ = 0.0001 is 2.25546\n"
     ]
    }
   ],
   "source": [
    "x_test_enc = data_prep(x_test,y_test)\n",
    "logits = x_test_enc @ weights[0.0001][0]\n",
    "counts = logits.exp() \n",
    "probs = counts / counts.sum(1,keepdim=True)\n",
    "test_loss = -probs[torch.arange(len(y_test)),y_test].log().mean() \n",
    "print(f'The test loss where λ = 0.0001 is {test_loss:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e75b8",
   "metadata": {},
   "source": [
    "The test loss where λ = 0.0001 is 2.25546\n",
    "\n",
    "The test loss where λ = 0 is 2.25543\n",
    "\n",
    "So the best loss is observed when there is no regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6d098c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "4d9eec25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0001    , 0.00021691, 0.00047048, 0.00102049, 0.0022135 ])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "array = np.geomspace(0.0001,0.5,12)\n",
    "array[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_enc = data_prep(x_val,y_val)\n",
    "logits = x_val_enc @ w\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1,keepdim=True)\n",
    "val_loss = -probs[torch.arange(len(y_val)),y_val].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "a08f681e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.6878,  2.6261,  0.5142,  0.2191,  0.7218,  1.7658, -0.8757, -0.8718,\n",
       "         1.1377,  1.5619, -0.5418, -0.2338,  1.3064,  1.0550,  0.0259,  1.9504,\n",
       "        -0.6824, -1.5299,  1.6672,  0.6739, -0.0419,  2.0518,  1.0370, -0.0371,\n",
       "        -0.7033,  1.1870,  0.9714], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e2f8fa2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.9848,  3.9991, -1.5889,  1.3663,  2.3685,  2.6098, -1.0464,  1.0785,\n",
       "        -1.0040,  3.5193, -0.2916, -0.2331,  1.1843, -1.1394,  4.1918,  1.0673,\n",
       "        -1.2495, -1.2209, -0.2065,  1.4142,  2.5037,  0.4799, -0.3422, -0.2361,\n",
       "        -1.2515,  1.5457,  0.7244], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val_enc = data_prep(x_val,y_val)\n",
    "logits = x_val_enc[0] @ w\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb7c450",
   "metadata": {},
   "source": [
    "## Excercise 4 \n",
    "\n",
    "Simply indexing into the rows of W, instead of one_hot encoding our inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4618f6b1",
   "metadata": {},
   "source": [
    "Doing a matrix mul between our one-hot-encoded x's and w, indexes into w, and plucks out the corresponding row.\n",
    "This is easy to visualize, when we think of a bigram.\n",
    "\n",
    "suppose \n",
    "```python\n",
    "x = [4,...]\n",
    "xenc = [0,0,0,1,0,0,0]\n",
    "w = []\n",
    "```\n",
    "what xenc@w will do is only pull out the 4th row (index 3) from w, as all else will be zeros.\n",
    "\n",
    "### We can achive the same by doing w[4]\n",
    "\n",
    "Now we try to do something similar with our trigrams, instead of each row having only one hot encoded value. It has 2, one for 2 previous characters.\n",
    "```python\n",
    "So if x = [ [1,2] , ... ]\n",
    "xenc = [ [0,1] [0,0,1] ]\n",
    "w = []\n",
    "\n",
    "now we can index into it using w[x[0,0] ] + w[27 + x[0,1]]4th\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**The important thing to note here is why do we do 27+? Because we want to select 2 rows of W. One is the 1st row and second is the 27+2 = 29th row. And add those up!**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df87422",
   "metadata": {},
   "source": [
    "### I've tried to explain the below, one step at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "044b265b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val_enc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "3930e0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21, 14])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "19d2cb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(21), tensor(14))"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val[0][0],x_val[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "f8cbeaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(21), tensor(41))"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val[0][0],x_val[0][1]+27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "f9667d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded = x_val_enc[0]@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "c6f3fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "indiced_into_w = w[x_val[0][0]] + w[x_val[0][1]+27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "1131eace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoded==indiced_into_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf029f8f",
   "metadata": {},
   "source": [
    "## Now implemented the whole thing at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3d56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.9848,  3.9991, -1.5889,  ..., -1.2515,  1.5457,  0.7244],\n",
       "        [ 2.9228,  3.1929, -0.2880,  ..., -2.0653,  2.4438, -0.6278],\n",
       "        [ 3.3723,  0.7481,  0.3578,  ...,  0.2730,  2.1822,  0.4783],\n",
       "        ...,\n",
       "        [ 5.3403,  3.5312, -1.4652,  ..., -1.9545,  1.9308,  1.4155],\n",
       "        [ 5.3403,  3.5312, -1.4652,  ..., -1.9545,  1.9308,  1.4155],\n",
       "        [ 4.4251,  4.0932,  0.6039,  ..., -1.9277,  3.8064, -0.0376]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch the first index rows of W\n",
    "# fetch the second index rows of W as well --> 27 + second row index\n",
    "# Add those up\n",
    "w[x_val[:,0]] + w[27+x_val[:,1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508d729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.9848,  3.9991, -1.5889,  ..., -1.2515,  1.5457,  0.7244],\n",
       "        [ 2.9228,  3.1929, -0.2880,  ..., -2.0653,  2.4438, -0.6278],\n",
       "        [ 3.3723,  0.7481,  0.3578,  ...,  0.2730,  2.1822,  0.4783],\n",
       "        ...,\n",
       "        [ 5.3403,  3.5312, -1.4652,  ..., -1.9545,  1.9308,  1.4155],\n",
       "        [ 5.3403,  3.5312, -1.4652,  ..., -1.9545,  1.9308,  1.4155],\n",
       "        [ 4.4251,  4.0932,  0.6039,  ..., -1.9277,  3.8064, -0.0376]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see both of these end up doing the same thing\n",
    "x_val_enc @ w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
