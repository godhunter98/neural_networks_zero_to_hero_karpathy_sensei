{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6df72150",
   "metadata": {},
   "source": [
    "# Exercise 2: Train, dev(validation), Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c1c6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('names.txt','r') as r:\n",
    "    words = r.read()\n",
    "    words = words.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19d4c045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a04ed1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = words[:int(len(words)* 0.8)]\n",
    "test = words[int(len(words)* 0.8):int(len(words)* 0.9)]\n",
    "val = words[int(len(words)* 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c260d53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80% training set\n",
      "20% validation set\n",
      "10% test set\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(train)/len(words)*100:.0f}% training set')\n",
    "print(f'{len(val)/len(words)*100:.0f}% validation set')\n",
    "print(f'{len(test)/len(words)*100:.0f}% test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a36e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars  = sorted(set(''.join(words)))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0067bded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos = {i:s for s,i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9871474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for w in words:\n",
    "    chars = ['.'] +list(w)+ ['.']\n",
    "    for char1,char2,char3 in zip(chars[:],chars[1:],chars[2:]):\n",
    "        ix1 = stoi[char1]\n",
    "        ix2 = stoi[char2]\n",
    "        ix3 = stoi[char3]\n",
    "        x.append([ix1,ix2])\n",
    "        y.append(ix3)\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0004255c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196113, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f7f25c",
   "metadata": {},
   "source": [
    "# Lets split our data into train,test and val. We use stratify = True to preserve the same class distribution as the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e882683",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_temp,y_train, y_temp = train_test_split(x,y,test_size=.2,random_state=40,shuffle=True,stratify=y)\n",
    "x_test, x_val,y_test,y_val = train_test_split(x_temp,y_temp,test_size=.5,random_state=40,shuffle=True,stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "364c50f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([156890, 2]) torch.Size([19612, 2]) torch.Size([19611, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,x_val.shape,x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b001126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(5)\n",
    "import torch.nn.functional as F\n",
    "def data_prep(x:torch.tensor,y:torch.tensor):\n",
    "    xenc = F.one_hot(x,num_classes=len(stoi)).float()\n",
    "    yenc = F.one_hot(y,num_classes=len(stoi)).float()\n",
    "\n",
    "    xflat = xenc.view(-1,27*2)\n",
    "\n",
    "    return xflat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94a92827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prep(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e0fd51",
   "metadata": {},
   "source": [
    "# Lets now run a few training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06785dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gradient descent with 250 steps: 100%|\u001b[32m██████████\u001b[0m| 250/250 [00:04<00:00, 56.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative_Log_Likelihood_loss after 250 runs is: 2.2428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "epochs = 250\n",
    "xenc = data_prep(x_train,y_train)\n",
    "\n",
    "w = torch.rand((27*2,27),generator=g,requires_grad=True)\n",
    "\n",
    "for n in trange(epochs,desc=f'Running gradient descent with {epochs} steps',colour='green'):\n",
    "\n",
    "    logits = xenc @ w\n",
    "\n",
    "    counts = logits.exp()\n",
    "\n",
    "    probs = counts / counts.sum(1,keepdim=True)\n",
    "\n",
    "    loss = -probs[torch.arange(len(y_train)),y_train].log().mean()\n",
    "\n",
    "    w.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    w.data += -(50*w.grad)\n",
    "\n",
    "    n+=1\n",
    "\n",
    "print(f'Negative_Log_Likelihood_loss after {n} runs is: {loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae232d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss on our validation and test set is 2.2511 and 2.2554\n"
     ]
    }
   ],
   "source": [
    "x_val_enc = data_prep(x_val,y_val)\n",
    "logits = x_val_enc @ w\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1,keepdim=True)\n",
    "val_loss = -probs[torch.arange(len(y_val)),y_val].log().mean()\n",
    "\n",
    "x_test_enc = data_prep(x_test,y_test)\n",
    "logits = x_test_enc @ w\n",
    "counts = logits.exp() \n",
    "probs = counts / counts.sum(1,keepdim=True)\n",
    "test_loss = -probs[torch.arange(len(y_test)),y_test].log().mean()\n",
    "\n",
    "\n",
    "print(f\"The loss on our validation and test set is {val_loss:.4f} and {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe7c9eb",
   "metadata": {},
   "source": [
    "# Exercise 3: Finding a good regularisation parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d7f3e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When x is 0.12, the test loss is 2.3792\n",
      "When x is 0.01, the test loss is 2.2678\n",
      "When x is 0.00, the test loss is 2.2567\n",
      "When x is 0.00, the test loss is 2.2556\n",
      "When x is 1.24, the test loss is 3.4928\n"
     ]
    }
   ],
   "source": [
    "x_test_enc = data_prep(x_test,y_test)\n",
    "logits = x_test_enc @ w\n",
    "counts = logits.exp() \n",
    "probs = counts / counts.sum(1,keepdim=True)\n",
    "def regularisation_term(w,x):\n",
    "    return x*(w**2).mean()\n",
    "\n",
    "for x in [0.1,0.01,0.001,0.0001,1]:\n",
    "    i = regularisation_term(w,x)\n",
    "    test_loss = -probs[torch.arange(len(y_test)),y_test].log().mean() + i\n",
    "    print(f\"When x is {i:.2f}, the test loss is {test_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad49c6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gradient descent with 250 steps: 100%|\u001b[32m██████████\u001b[0m| 250/250 [00:04<00:00, 55.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative_Log_Likelihood_loss after 250 runs is: 2.2428\n",
      "Negative_Log_Likelihood_loss on validattion set is: 2.2511 when lambda is 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gradient descent with 250 steps: 100%|\u001b[32m██████████\u001b[0m| 250/250 [00:04<00:00, 58.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative_Log_Likelihood_loss after 250 runs is: 2.5449\n",
      "Negative_Log_Likelihood_loss on validattion set is: 2.3951 when lambda is 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running gradient descent with 250 steps:  44%|\u001b[32m████▍     \u001b[0m| 111/250 [00:01<00:02, 57.06it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mprobs[torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(y_train)),y_train]\u001b[38;5;241m.\u001b[39mlog()\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m+\u001b[39m l\u001b[38;5;241m*\u001b[39m(w\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     23\u001b[0m w\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     26\u001b[0m w\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(\u001b[38;5;241m50\u001b[39m\u001b[38;5;241m*\u001b[39mw\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m     28\u001b[0m n\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    650\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[1;32m    354\u001b[0m     tensors,\n\u001b[1;32m    355\u001b[0m     grad_tensors_,\n\u001b[1;32m    356\u001b[0m     retain_graph,\n\u001b[1;32m    357\u001b[0m     create_graph,\n\u001b[1;32m    358\u001b[0m     inputs,\n\u001b[1;32m    359\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    360\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    361\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lambdas = [0] + [1/10**i for i in range(7)]\n",
    "g = torch.Generator().manual_seed(5)\n",
    "weights = {}\n",
    "\n",
    "for l in lambdas:\n",
    "\n",
    "    from tqdm import trange\n",
    "    epochs = 250\n",
    "    xenc = data_prep(x_train,y_train)\n",
    "\n",
    "    w = torch.rand((27*2,27),generator=g,requires_grad=True)\n",
    "\n",
    "    for n in trange(epochs,desc=f'Running gradient descent with {epochs} steps',colour='green'):\n",
    "\n",
    "        logits = xenc @ w\n",
    "\n",
    "        counts = logits.exp()\n",
    "\n",
    "        probs = counts / counts.sum(1,keepdim=True)\n",
    "\n",
    "        loss = -probs[torch.arange(len(y_train)),y_train].log().mean() + l*(w**2).mean()\n",
    "\n",
    "        w.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        w.data += -(50*w.grad)\n",
    "\n",
    "        n+=1\n",
    "\n",
    "    x_test_enc = data_prep(x_val,y_val)\n",
    "    logits = x_test_enc @ w\n",
    "    counts = logits.exp() # This exponentiates to remove all -ve values.\n",
    "    probs = counts / counts.sum(1,keepdim=True) # The above 2 lines, manually implement a soft max. Now we only have items btw 0 and 1, all rows sum to 1.\n",
    "    \n",
    "    # Below, we've implemented an average log-likelihood loss.\n",
    "    test_loss = -probs[torch.arange(len(y_val)),y_val].log().mean() \n",
    "    \n",
    "    weights[l] = [w,test_loss]\n",
    "\n",
    "    print(f'Negative_Log_Likelihood_loss after {n} runs is: {loss:.4f}')\n",
    "    print(f'Negative_Log_Likelihood_loss on validattion set is: {test_loss :.4f} when lambda is {l:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5767b103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best λ : 0.0001\n",
      "dev CE : 2.2509894371032715\n"
     ]
    }
   ],
   "source": [
    "best_lam, (best_w, best_loss_tensor) = min(\n",
    "    weights.items(),\n",
    "    key=lambda kv: kv[1][1].item()   # kv[1] == [w, loss]; kv[1][1] is the loss tensor\n",
    ")\n",
    "\n",
    "best_loss = best_loss_tensor.item()\n",
    "print(\"best λ :\", best_lam)\n",
    "print(\"dev CE :\", best_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc6945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test loss where λ = 0.0001 is 2.25546\n"
     ]
    }
   ],
   "source": [
    "x_test_enc = data_prep(x_test,y_test)\n",
    "logits = x_test_enc @ weights[0.0001][0]\n",
    "counts = logits.exp() \n",
    "probs = counts / counts.sum(1,keepdim=True)\n",
    "test_loss = -probs[torch.arange(len(y_test)),y_test].log().mean() \n",
    "print(f'The test loss where λ = 0.0001 is {test_loss:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e75b8",
   "metadata": {},
   "source": [
    "The test loss where λ = 0.0001 is 2.25546\n",
    "\n",
    "The test loss where λ = 0 is 2.25543\n",
    "\n",
    "So the best loss is observed when there is no regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6d098c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9eec25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0001    , 0.00021691, 0.00047048, 0.00102049, 0.0022135 ])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "array = np.geomspace(0.0001,0.5,12)\n",
    "array[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_enc = data_prep(x_val,y_val)\n",
    "logits = x_val_enc @ w\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1,keepdim=True)\n",
    "val_loss = -probs[torch.arange(len(y_val)),y_val].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f681e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.6878,  2.6261,  0.5142,  0.2191,  0.7218,  1.7658, -0.8757, -0.8718,\n",
       "         1.1377,  1.5619, -0.5418, -0.2338,  1.3064,  1.0550,  0.0259,  1.9504,\n",
       "        -0.6824, -1.5299,  1.6672,  0.6739, -0.0419,  2.0518,  1.0370, -0.0371,\n",
       "        -0.7033,  1.1870,  0.9714], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f8fa2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.9848,  3.9991, -1.5889,  1.3663,  2.3685,  2.6098, -1.0464,  1.0785,\n",
       "        -1.0040,  3.5193, -0.2916, -0.2331,  1.1843, -1.1394,  4.1918,  1.0673,\n",
       "        -1.2495, -1.2209, -0.2065,  1.4142,  2.5037,  0.4799, -0.3422, -0.2361,\n",
       "        -1.2515,  1.5457,  0.7244], grad_fn=<SqueezeBackward4>)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val_enc = data_prep(x_val,y_val)\n",
    "logits = x_val_enc[0] @ w\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb7c450",
   "metadata": {},
   "source": [
    "## Excercise 4 \n",
    "\n",
    "Simply indexing into the rows of W, instead of one_hot encoding our inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4618f6b1",
   "metadata": {},
   "source": [
    "Doing a matrix mul between our one-hot-encoded x's and w, indexes into w, and plucks out the corresponding row.\n",
    "This is easy to visualize, when we think of a bigram.\n",
    "\n",
    "suppose \n",
    "```python\n",
    "x = [4,...]\n",
    "xenc = [0,0,0,1,0,0,0]\n",
    "w = []\n",
    "```\n",
    "what xenc@w will do is only pull out the 4th row (index 3) from w, as all else will be zeros.\n",
    "\n",
    "### We can achive the same by doing w[4]\n",
    "\n",
    "Now we try to do something similar with our trigrams, instead of each row having only one hot encoded value. It has 2, one for 2 previous characters.\n",
    "```python\n",
    "So if x = [ [1,2] , ... ]\n",
    "xenc = [ [0,1] [0,0,1] ]\n",
    "w = []\n",
    "\n",
    "now we can index into it using w[x[0,0] ] + w[27 + x[0,1]]4th\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**The important thing to note here is why do we do 27+? Because we want to select 2 rows of W. One is the 1st row and second is the 27+2 = 29th row. And add those up!**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df87422",
   "metadata": {},
   "source": [
    "### I've tried to explain the below, one step at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044b265b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val_enc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930e0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21, 14])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d2cb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(21), tensor(14))"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val[0][0],x_val[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cbeaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(21), tensor(41))"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val[0][0],x_val[0][1]+27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9667d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded = x_val_enc[0]@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "indiced_into_w = w[x_val[0][0]] + w[x_val[0][1]+27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1131eace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoded==indiced_into_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf029f8f",
   "metadata": {},
   "source": [
    "## Now implemented the whole thing at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87a3d56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19612, 27])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch the first index rows of W\n",
    "# fetch the second index rows of W as well --> 27 + second row index\n",
    "# Add those up\n",
    "(w[x_val[:,0]] + w[27+x_val[:,1]]).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508d729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.9848,  3.9991, -1.5889,  ..., -1.2515,  1.5457,  0.7244],\n",
       "        [ 2.9228,  3.1929, -0.2880,  ..., -2.0653,  2.4438, -0.6278],\n",
       "        [ 3.3723,  0.7481,  0.3578,  ...,  0.2730,  2.1822,  0.4783],\n",
       "        ...,\n",
       "        [ 5.3403,  3.5312, -1.4652,  ..., -1.9545,  1.9308,  1.4155],\n",
       "        [ 5.3403,  3.5312, -1.4652,  ..., -1.9545,  1.9308,  1.4155],\n",
       "        [ 4.4251,  4.0932,  0.6039,  ..., -1.9277,  3.8064, -0.0376]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see both of these end up doing the same thing\n",
    "x_val_enc @ w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
